{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c28d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 14:18:37.091504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/reshma/Documents/Coding/capstone_project/attack/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/reshma/Documents/Coding/capstone_project/attack/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/reshma/Documents/Coding/capstone_project/attack/lib/python3.9/site-packages/art/estimators/certification/__init__.py:30: UserWarning: PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\n",
      "  warnings.warn(\"PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'CIC-Darknet2020.csv'\n",
    "TARGET_LABELS = ['Tor', 'Non-Tor', 'VPN', 'NonVPN']\n",
    "\n",
    "BASELINE_MODEL_PATH = 'model-multi.h5'\n",
    "SCALER_PATH = 'scaler-multi.pkl'\n",
    "DEFENDED_MODEL_PATH = 'model-defended-v2.h5'\n",
    "\n",
    "ADV_DATA_PATH = 'X_train_adv-v2.npy'\n",
    "ADV_LABELS_PATH = 'y_train_adv_labels_ohe-v2.npy'\n",
    "ADV_LABELS_ENCODED_PATH = 'y_train_adv_labels_encoded-v2.npy'\n",
    "\n",
    "PGD_EPS = 0.1\n",
    "PGD_EPS_STEP = 0.01\n",
    "PGD_MAX_ITER = 40\n",
    "PGD_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{DATA_PATH}' not found.\")\n",
    "        return None\n",
    "\n",
    "    df.columns = [*df.columns[:-2], 'Label', 'Label_Type']\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df_multi = df[df['Label'].isin(TARGET_LABELS)].copy()\n",
    "\n",
    "    non_feature_cols = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Label', 'Label_Type']\n",
    "    X = df_multi.drop(columns=non_feature_cols).apply(pd.to_numeric)\n",
    "    y = df_multi['Label']\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    n_classes = len(le.classes_)\n",
    "    target_names = le.classes_\n",
    "    y_ohe = to_categorical(y_encoded, num_classes=n_classes)\n",
    "    \n",
    "    print(\"--- Class Encoding Mapping ---\")\n",
    "    for index, label in enumerate(le.classes_):\n",
    "        print(f\"Class Index {index} -> {label}\")\n",
    "\n",
    "    X_train, X_test, y_train_ohe, y_test_ohe, y_train_encoded, y_test_encoded = train_test_split(\n",
    "        X, y_ohe, y_encoded,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "        print(f\"\\nScaler '{SCALER_PATH}' loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{SCALER_PATH}' not found. Did you run the baseline notebook?\")\n",
    "        return None\n",
    "        \n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # --- Reshape for CNN ---\n",
    "    n_features = X_test_scaled.shape[1]\n",
    "    X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], n_features, 1))\n",
    "    X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], n_features, 1))\n",
    "    \n",
    "    print(f\"Data preparation complete. Found {n_features} features and {n_classes} classes.\")\n",
    "    \n",
    "    return X_train_cnn, y_train_ohe, y_train_encoded, X_test_cnn, y_test_ohe, y_test_encoded, target_names, n_features, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919eb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art_classifier_multi(model_path, n_features, n_classes):\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "        print(f\"Model '{model_path}' loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Model file '{model_path}' not found or failed to load.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None, None\n",
    "        \n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    classifier = TensorFlowV2Classifier(\n",
    "        model=model,\n",
    "        loss_object=loss_object,\n",
    "        input_shape=(n_features, 1),\n",
    "        nb_classes=n_classes,\n",
    "        channels_first=False\n",
    "    )\n",
    "    return model, classifier\n",
    "\n",
    "def create_multi_class_cnn(input_shape, n_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(\"New, un-trained CNN model created.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230522c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Class Encoding Mapping ---\n",
      "Class Index 0 -> Non-Tor\n",
      "Class Index 1 -> NonVPN\n",
      "Class Index 2 -> Tor\n",
      "Class Index 3 -> VPN\n",
      "\n",
      "Scaler 'scaler-multi.pkl' loaded successfully.\n",
      "Data preparation complete. Found 76 features and 4 classes.\n",
      "\n",
      "Loading baseline model 'model-multi.h5' to create attacks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'model-multi.h5' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "data = load_all_data()\n",
    "\n",
    "if data:\n",
    "    X_train_cnn, y_train_ohe, y_train_encoded, \\\n",
    "    X_test_cnn, y_test_ohe, y_test_encoded, \\\n",
    "    target_names, n_features, n_classes = data\n",
    "    \n",
    "    X_train_art = X_train_cnn.astype(np.float32)\n",
    "    \n",
    "    print(f\"\\nLoading baseline model '{BASELINE_MODEL_PATH}' to create attacks...\")\n",
    "    baseline_model, classifier = get_art_classifier_multi(BASELINE_MODEL_PATH, n_features, n_classes)\n",
    "else:\n",
    "    print(\"Data loading failed. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb97a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 1: No existing data found. Generating STRONGER PGD attack...\n",
      "This will be slow (max_iter=40). Attacking 126852 samples...\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PGD - Batches: 1983it [1:16:42,  2.03s/it]2025-11-01 15:35:46.582298: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adversarial training data generated.\n",
      "Saving data to: X_train_adv-v2.npy\n",
      "Saving labels to: y_train_adv_labels_ohe-v2.npy\n",
      "Saving encoded labels to: y_train_adv_labels_encoded-v2.npy\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(ADV_DATA_PATH) and os.path.exists(ADV_LABELS_PATH):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"STEP 1: Found existing adversarial data. Loading files...\")\n",
    "    X_train_adv = np.load(ADV_DATA_PATH)\n",
    "    y_train_ohe_adv = np.load(ADV_LABELS_PATH)\n",
    "    y_train_encoded_adv = np.load(ADV_LABELS_ENCODED_PATH)\n",
    "    print(\"Adversarial training data loaded from local files.\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"STEP 1: No existing data found. Generating STRONGER PGD attack...\")\n",
    "    print(f\"This will be slow (max_iter=40). Attacking {len(X_train_art)} samples...\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    if 'classifier' not in locals():\n",
    "        print(\"Error: Classifier not loaded. Cannot generate attacks.\")\n",
    "    else:\n",
    "        attack = ProjectedGradientDescent(\n",
    "            classifier,\n",
    "            eps=PGD_EPS,\n",
    "            eps_step=PGD_EPS_STEP,\n",
    "            max_iter=PGD_MAX_ITER,\n",
    "            random_eps=True,\n",
    "            batch_size=PGD_BATCH_SIZE,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        X_train_adv = attack.generate(x=X_train_art, y=y_train_ohe)\n",
    "        \n",
    "        print(\"\\nAdversarial training data generated.\")\n",
    "        print(f\"Saving data to: {ADV_DATA_PATH}\")\n",
    "        np.save(ADV_DATA_PATH, X_train_adv)\n",
    "        \n",
    "        print(f\"Saving labels to: {ADV_LABELS_PATH}\")\n",
    "        np.save(ADV_LABELS_PATH, y_train_ohe)\n",
    "        y_train_ohe_adv = y_train_ohe\n",
    "        \n",
    "        print(f\"Saving encoded labels to: {ADV_LABELS_ENCODED_PATH}\")\n",
    "        np.save(ADV_LABELS_ENCODED_PATH, y_train_encoded)\n",
    "        y_train_encoded_adv = y_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 2: Creating and shuffling hardened dataset...\n",
      "==================================================\n",
      "\n",
      "Original training data shape: (126852, 76, 1)\n",
      "New hardened training data shape: (253704, 76, 1)\n",
      "New hardened labels shape: (253704, 4)\n",
      "\n",
      "Hardened dataset created and shuffled.\n"
     ]
    }
   ],
   "source": [
    "if 'X_train_adv' in locals():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 2: Creating and shuffling hardened dataset...\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    X_hardened = np.concatenate((X_train_art, X_train_adv), axis=0)\n",
    "    y_hardened_ohe = np.concatenate((y_train_ohe, y_train_ohe_adv), axis=0)\n",
    "    y_hardened_encoded = np.concatenate((y_train_encoded, y_train_encoded_adv), axis=0)\n",
    "    \n",
    "    print(f\"Original training data shape: {X_train_art.shape}\")\n",
    "    print(f\"New hardened training data shape: {X_hardened.shape}\")\n",
    "    print(f\"New hardened labels shape: {y_hardened_ohe.shape}\")\n",
    "\n",
    "    X_hardened, y_hardened_ohe, y_hardened_encoded = shuffle(\n",
    "        X_hardened, y_hardened_ohe, y_hardened_encoded, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\nHardened dataset created and shuffled.\")\n",
    "\n",
    "else:\n",
    "    print(\"Adversarial training data not found. Skipping dataset creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88dfa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 3: Training the new DEFENDED model...\n",
      "This will take approx. 2x as long as the original training.\n",
      "==================================================\n",
      "\n",
      "New, un-trained CNN model created.\n",
      "\n",
      "Class weights for new dataset:\n",
      "Weight for class 0 (Non-Tor): 0.36\n",
      "Weight for class 1 (NonVPN): 1.66\n",
      "Weight for class 2 (Tor): 28.49\n",
      "Weight for class 3 (VPN): 1.73\n",
      "Epoch 1/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 19ms/step - accuracy: 0.6623 - loss: 0.8151 - val_accuracy: 0.8855 - val_loss: 0.4380\n",
      "Epoch 2/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 19ms/step - accuracy: 0.9186 - loss: 0.3986 - val_accuracy: 0.9151 - val_loss: 0.2495\n",
      "Epoch 3/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 19ms/step - accuracy: 0.9387 - loss: 0.3010 - val_accuracy: 0.9219 - val_loss: 0.2155\n",
      "Epoch 4/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 19ms/step - accuracy: 0.9472 - loss: 0.2495 - val_accuracy: 0.9387 - val_loss: 0.1839\n",
      "Epoch 5/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 21ms/step - accuracy: 0.9547 - loss: 0.2162 - val_accuracy: 0.9262 - val_loss: 0.2361\n",
      "Epoch 6/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 20ms/step - accuracy: 0.9586 - loss: 0.1999 - val_accuracy: 0.9309 - val_loss: 0.2003\n",
      "Epoch 7/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 20ms/step - accuracy: 0.9620 - loss: 0.1791 - val_accuracy: 0.9418 - val_loss: 0.1627\n",
      "Epoch 8/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 20ms/step - accuracy: 0.9652 - loss: 0.1661 - val_accuracy: 0.9474 - val_loss: 0.1513\n",
      "Epoch 9/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 20ms/step - accuracy: 0.9665 - loss: 0.1615 - val_accuracy: 0.9493 - val_loss: 0.1441\n",
      "Epoch 10/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 20ms/step - accuracy: 0.9682 - loss: 0.1535 - val_accuracy: 0.9472 - val_loss: 0.1433\n",
      "Epoch 11/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 20ms/step - accuracy: 0.9690 - loss: 0.1447 - val_accuracy: 0.9467 - val_loss: 0.1434\n",
      "Epoch 12/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 21ms/step - accuracy: 0.9695 - loss: 0.1446 - val_accuracy: 0.9491 - val_loss: 0.1358\n",
      "Epoch 13/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 21ms/step - accuracy: 0.9695 - loss: 0.1498 - val_accuracy: 0.9550 - val_loss: 0.1257\n",
      "Epoch 14/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 21ms/step - accuracy: 0.9722 - loss: 0.1366 - val_accuracy: 0.9468 - val_loss: 0.1494\n",
      "Epoch 15/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 22ms/step - accuracy: 0.9724 - loss: 0.1320 - val_accuracy: 0.9552 - val_loss: 0.1268\n",
      "Epoch 16/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 21ms/step - accuracy: 0.9733 - loss: 0.1313 - val_accuracy: 0.9536 - val_loss: 0.1227\n",
      "Epoch 17/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 22ms/step - accuracy: 0.9723 - loss: 0.1360 - val_accuracy: 0.9471 - val_loss: 0.1342\n",
      "Epoch 18/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 21ms/step - accuracy: 0.9747 - loss: 0.1202 - val_accuracy: 0.9496 - val_loss: 0.1305\n",
      "Epoch 19/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 22ms/step - accuracy: 0.9729 - loss: 0.1257 - val_accuracy: 0.9510 - val_loss: 0.1281\n",
      "Epoch 20/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 21ms/step - accuracy: 0.9739 - loss: 0.1200 - val_accuracy: 0.9442 - val_loss: 0.1354\n",
      "Epoch 21/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 20ms/step - accuracy: 0.9744 - loss: 0.1220 - val_accuracy: 0.9503 - val_loss: 0.1357\n",
      "Epoch 22/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 21ms/step - accuracy: 0.9745 - loss: 0.1195 - val_accuracy: 0.9504 - val_loss: 0.1312\n",
      "Epoch 23/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 21ms/step - accuracy: 0.9738 - loss: 0.1224 - val_accuracy: 0.9551 - val_loss: 0.1190\n",
      "Epoch 24/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 22ms/step - accuracy: 0.9744 - loss: 0.1216 - val_accuracy: 0.9591 - val_loss: 0.1122\n",
      "Epoch 25/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 22ms/step - accuracy: 0.9754 - loss: 0.1179 - val_accuracy: 0.9481 - val_loss: 0.1367\n",
      "Epoch 26/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 20ms/step - accuracy: 0.9753 - loss: 0.1177 - val_accuracy: 0.9535 - val_loss: 0.1254\n",
      "Epoch 27/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 26ms/step - accuracy: 0.9755 - loss: 0.1127 - val_accuracy: 0.9590 - val_loss: 0.1138\n",
      "Epoch 28/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 24ms/step - accuracy: 0.9758 - loss: 0.1136 - val_accuracy: 0.9541 - val_loss: 0.1195\n",
      "Epoch 29/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 22ms/step - accuracy: 0.9753 - loss: 0.1173 - val_accuracy: 0.9539 - val_loss: 0.1223\n",
      "Epoch 30/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 23ms/step - accuracy: 0.9755 - loss: 0.1185 - val_accuracy: 0.9562 - val_loss: 0.1131\n",
      "Epoch 31/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 22ms/step - accuracy: 0.9757 - loss: 0.1159 - val_accuracy: 0.9574 - val_loss: 0.1127\n",
      "Epoch 32/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 22ms/step - accuracy: 0.9759 - loss: 0.1159 - val_accuracy: 0.9526 - val_loss: 0.1241\n",
      "Epoch 33/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 22ms/step - accuracy: 0.9768 - loss: 0.1084 - val_accuracy: 0.9459 - val_loss: 0.1463\n",
      "Epoch 34/50\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 23ms/step - accuracy: 0.9759 - loss: 0.1114 - val_accuracy: 0.9523 - val_loss: 0.1257\n",
      "\n",
      "Defended model training complete.\n"
     ]
    }
   ],
   "source": [
    "if 'X_hardened' in locals():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 3: Training the new DEFENDED model...\")\n",
    "    print(\"This will take approx. 2x as long as the original training.\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    input_shape = (n_features, 1)\n",
    "    model_defended = create_multi_class_cnn(input_shape, n_classes)\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_hardened_encoded),\n",
    "        y=y_hardened_encoded\n",
    "    )\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(\"\\nClass weights for new dataset:\")\n",
    "    for i in range(n_classes):\n",
    "          print(f\"Weight for class {i} ({target_names[i]}): {class_weights_dict[i]:.2f}\")\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                                 patience=10,\n",
    "                                 mode='max',\n",
    "                                 restore_best_weights=True)\n",
    "    \n",
    "    history = model_defended.fit(\n",
    "        X_hardened, y_hardened_ohe,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test_cnn, y_test_ohe), # Validate on clean test data\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDefended model training complete.\")\n",
    "\n",
    "else:\n",
    "    print(\"Hardened dataset not found. Skipping model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a0710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 4: Saving and evaluating the defended model...\n",
      "==================================================\n",
      "\n",
      "New defended model saved to 'model-defended-v2.h5'\n",
      "\n",
      "Evaluating defended model on CLEAN test data:\n",
      "Clean Test Accuracy (Defended): 95.9135%\n",
      "Clean Test Loss (Defended): 0.1122\n",
      "\u001b[1m992/992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step\n",
      "\n",
      "Classification Report (Defended Model on Clean Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Non-Tor       1.00      0.99      1.00     22079\n",
      "      NonVPN       0.88      0.86      0.87      4772\n",
      "         Tor       0.90      0.87      0.89       279\n",
      "         VPN       0.86      0.90      0.88      4584\n",
      "\n",
      "    accuracy                           0.96     31714\n",
      "   macro avg       0.91      0.91      0.91     31714\n",
      "weighted avg       0.96      0.96      0.96     31714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'model_defended' in locals():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 4: Saving and evaluating the defended model...\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    model_defended.save(DEFENDED_MODEL_PATH)\n",
    "    print(f\"New defended model saved to '{DEFENDED_MODEL_PATH}'\")\n",
    "\n",
    "    print(\"\\nEvaluating defended model on CLEAN test data:\")\n",
    "    loss, accuracy = model_defended.evaluate(X_test_cnn, y_test_ohe, verbose=0)\n",
    "    \n",
    "    print(f\"Clean Test Accuracy (Defended): {accuracy * 100:.4f}%\")\n",
    "    print(f\"Clean Test Loss (Defended): {loss:.4f}\")\n",
    "\n",
    "    y_pred_probs = model_defended.predict(X_test_cnn)\n",
    "    y_pred_encoded = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    print(\"\\nClassification Report (Defended Model on Clean Data):\")\n",
    "    print(classification_report(y_test_encoded, y_pred_encoded, target_names=target_names))\n",
    "\n",
    "else:\n",
    "    print(\"Defended model not found. Skipping save/evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

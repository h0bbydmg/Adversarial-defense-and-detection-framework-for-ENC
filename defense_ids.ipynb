{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abbbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model, Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, GaussianNoise, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd023b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defense classes defined.\n"
     ]
    }
   ],
   "source": [
    "class FeatureSqueezing:\n",
    "    def __init__(self, bit_depth=5):\n",
    "        self.bit_depth = bit_depth\n",
    "        self.max_val = 2 ** self.bit_depth - 1\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x_int = np.round(x * self.max_val)\n",
    "        x_squeezed = x_int / self.max_val\n",
    "        return x_squeezed\n",
    "\n",
    "def build_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = GaussianNoise(0.05)(input_layer) \n",
    "    encoded = Dense(64)(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Activation('relu')(encoded)\n",
    "    encoded = Dense(32)(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Activation('relu')(encoded)\n",
    "    decoded = Dense(64)(encoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Activation('relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse') # Mean Squared Error\n",
    "    return autoencoder\n",
    "\n",
    "print(\"Defense classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ef2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clean data for Autoencoder training...\n",
      "Data loaded. Shape: (210876, 78, 1)\n",
      "\n",
      "Training Denoising Autoencoder...\n",
      "Epoch 1/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2885 - val_loss: 0.1609\n",
      "Epoch 2/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1171 - val_loss: 0.1112\n",
      "Epoch 3/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0922 - val_loss: 0.0985\n",
      "Epoch 4/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0802 - val_loss: 0.0741\n",
      "Epoch 5/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0714 - val_loss: 0.0593\n",
      "Epoch 6/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0650 - val_loss: 0.0449\n",
      "Epoch 7/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0594 - val_loss: 0.0367\n",
      "Epoch 8/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0540 - val_loss: 0.0393\n",
      "Epoch 9/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0502 - val_loss: 0.0380\n",
      "Epoch 10/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0470 - val_loss: 0.0418\n",
      "Epoch 11/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0442 - val_loss: 0.0509\n",
      "Epoch 12/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0422 - val_loss: 0.0456\n",
      "Epoch 13/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0391 - val_loss: 0.0637\n",
      "Epoch 14/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0369 - val_loss: 0.0860\n",
      "Epoch 15/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0349 - val_loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Autoencoder trained and saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading clean data for Autoencoder training...\")\n",
    "\n",
    "try:\n",
    "    X_clean = np.load('X_test_ids.npy')\n",
    "    print(f\"Data loaded. Shape: {X_clean.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'X_test_ids.npy' not found. Please run the save code in your Baseline Notebook.\")\n",
    "    raise\n",
    "\n",
    "n_samples = X_clean.shape[0]\n",
    "n_features = X_clean.shape[1]\n",
    "X_flat = X_clean.reshape(n_samples, n_features)\n",
    "\n",
    "print(\"\\nTraining Denoising Autoencoder...\")\n",
    "X_train_ae = X_flat[:50000]\n",
    "\n",
    "autoencoder = build_autoencoder(n_features)\n",
    "history = autoencoder.fit(\n",
    "    X_train_ae, X_train_ae,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "autoencoder.save('ids_autoencoder.h5')\n",
    "print(\"✓ Autoencoder trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ef711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline classifier...\n",
      "\n",
      "============================================================\n",
      "COMBINED DEFENSE EVALUATION (Autoencoder + Squeezing)\n",
      "============================================================\n",
      "Attack     No Defense      Defended        Improvement    \n",
      "------------------------------------------------------------\n",
      "FGSM       62.29          % 77.35          % +15.06          %\n",
      "PGD        34.43          % 75.33          % +40.90          %\n",
      "C&W        44.92          % 81.20          % +36.28          %\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading baseline classifier...\")\n",
    "classifier = load_model('ids_model.h5')\n",
    "\n",
    "squeezer = FeatureSqueezing(bit_depth=5)\n",
    "\n",
    "def apply_combined_defense(x_input):\n",
    "    x_flat = x_input.reshape(x_input.shape[0], -1)\n",
    "    x_denoised = autoencoder.predict(x_flat, verbose=0)\n",
    "    x_squeezed = squeezer(x_denoised)\n",
    "    \n",
    "    return x_squeezed.reshape(x_input.shape)\n",
    "\n",
    "attack_files = {\n",
    "    'FGSM': ('X_adv_fgsm_ids.npy', 'y_test_fgsm_ids_int.npy'),\n",
    "    'PGD':  ('X_adv_pgd_ids.npy',  'y_test_pgd_ids_int.npy'),\n",
    "    'C&W':  ('X_adv_cw_ids.npy',   'y_test_cw_ids_int.npy')\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINED DEFENSE EVALUATION (Autoencoder + Squeezing)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Attack':<10} {'No Defense':<15} {'Defended':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for attack_name, (x_path, y_path) in attack_files.items():\n",
    "    try:\n",
    "        if not os.path.exists(x_path):\n",
    "            print(f\"{attack_name:<10} -- Skipped (File not found) --\")\n",
    "            continue\n",
    "            \n",
    "        X_adv = np.load(x_path)\n",
    "        y_true = np.load(y_path)\n",
    "        y_pred_raw = np.argmax(classifier.predict(X_adv, verbose=0), axis=1)\n",
    "        acc_raw = accuracy_score(y_true, y_pred_raw) * 100\n",
    "        \n",
    "        X_defended = apply_combined_defense(X_adv)\n",
    "        \n",
    "        y_pred_def = np.argmax(classifier.predict(X_defended, verbose=0), axis=1)\n",
    "        acc_def = accuracy_score(y_true, y_pred_def) * 100\n",
    "        \n",
    "        improvement = acc_def - acc_raw\n",
    "        results[attack_name] = (acc_raw, acc_def)\n",
    "        \n",
    "        print(f\"{attack_name:<10} {acc_raw:<15.2f}% {acc_def:<15.2f}% +{improvement:<15.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {attack_name}: {e}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a710131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline classifier...\n",
      "\n",
      "============================================================\n",
      "COMBINED DEFENSE EVALUATION (Autoencoder + Squeezing)\n",
      "============================================================\n",
      "Attack     No Defense      Defended        Improvement    \n",
      "------------------------------------------------------------\n",
      "FGSM       62.29          % 77.35          % +15.06          %\n",
      "PGD        34.43          % 75.33          % +40.90          %\n",
      "C&W        44.92          % 81.20          % +36.28          %\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading baseline classifier...\")\n",
    "classifier = load_model('ids_model.h5')\n",
    "\n",
    "squeezer = FeatureSqueezing(bit_depth=5)\n",
    "\n",
    "def apply_combined_defense(x_input):\n",
    "    x_flat = x_input.reshape(x_input.shape[0], -1)\n",
    "    x_denoised = autoencoder.predict(x_flat, verbose=0)\n",
    "    x_squeezed = squeezer(x_denoised)\n",
    "    return x_squeezed.reshape(x_input.shape)\n",
    "\n",
    "attack_files = {\n",
    "    'FGSM': ('X_adv_fgsm_ids.npy', 'y_test_fgsm_ids_int.npy'),\n",
    "    'PGD':  ('X_adv_pgd_ids.npy',  'y_test_pgd_ids_int.npy'),\n",
    "    'C&W':  ('X_adv_cw_ids.npy',   'y_test_cw_ids_int.npy')\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINED DEFENSE EVALUATION (Autoencoder + Squeezing)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Attack':<10} {'No Defense':<15} {'Defended':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for attack_name, (x_path, y_path) in attack_files.items():\n",
    "    try:\n",
    "        if not os.path.exists(x_path):\n",
    "            print(f\"{attack_name:<10} -- Skipped (File not found) --\")\n",
    "            continue\n",
    "            \n",
    "        X_adv = np.load(x_path)\n",
    "        y_true = np.load(y_path)\n",
    "\n",
    "        y_pred_raw = np.argmax(classifier.predict(X_adv, verbose=0), axis=1)\n",
    "        acc_raw = accuracy_score(y_true, y_pred_raw) * 100\n",
    "        \n",
    "        X_defended = apply_combined_defense(X_adv)\n",
    "        \n",
    "        y_pred_def = np.argmax(classifier.predict(X_defended, verbose=0), axis=1)\n",
    "        acc_def = accuracy_score(y_true, y_pred_def) * 100\n",
    "        \n",
    "        improvement = acc_def - acc_raw\n",
    "        results[attack_name] = (acc_raw, acc_def)\n",
    "        \n",
    "        print(f\"{attack_name:<10} {acc_raw:<15.2f}% {acc_def:<15.2f}% +{improvement:<15.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {attack_name}: {e}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
